{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model, parameters, performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import utils\n",
    "from models.vqvae import VQVAE\n",
    "import os\n",
    "import shutil\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for argument values\n",
    "BATCH_SIZE = 32\n",
    "N_UPDATES = 5000\n",
    "N_HIDDENS = 128  # Updated to 128\n",
    "N_RESIDUAL_HIDDENS = 64  # Updated to 64\n",
    "N_RESIDUAL_LAYERS = 3  # Updated to 3\n",
    "EMBEDDING_DIM = 128  # Updated to 128\n",
    "N_EMBEDDINGS = 512\n",
    "BETA = 0.25\n",
    "LEARNING_RATE = 3e-4\n",
    "LOG_INTERVAL = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GravitationalHackathonDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads gravitational hackathon dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path, train=True):\n",
    "        if file_path.split('.')[-1]=='npz':\n",
    "            data = np.load(file_path, allow_pickle=True)['data']\n",
    "            data = np.random.permutation(data)\n",
    "        else:\n",
    "            data = np.load(file_path)\n",
    "        stds = np.std(data, axis=-1)[:, :, np.newaxis]\n",
    "        data = data/stds\n",
    "        if train:\n",
    "            np.save(\"backgroundtest.npy\", data[-20000:])\n",
    "            data = data[:-20000].reshape(-1, 1, data.shape[-2], data.shape[-1])\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        label = 0\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hackathon(filepath):\n",
    "    \n",
    "    train = GravitationalHackathonDataset(filepath)\n",
    "    print(train.data.shape)\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_results(model, results, timestamp):\n",
    "    SAVE_MODEL_PATH = os.getcwd() + '/results'\n",
    "\n",
    "    results_to_save = {\n",
    "        'model': model.state_dict(),\n",
    "        'results': results,\n",
    "    }\n",
    "    torch.save(results_to_save,\n",
    "               SAVE_MODEL_PATH + '/vqvae_data_' + timestamp + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 1, 2, 200)\n",
      "1.000476133886439\n"
     ]
    }
   ],
   "source": [
    "data_folder_path = os.getcwd()\n",
    "data_file_path = data_folder_path + '/background.npz'\n",
    "training_data = load_hackathon(filepath=data_file_path)\n",
    "training_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "x_train_var = np.var(training_data.data)\n",
    "print(x_train_var)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One residual layer inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_dim, res_h_dim, kernel_size=3,\n",
    "                      stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(res_h_dim, h_dim, kernel_size=1,\n",
    "                      stride=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.res_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual layers inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim, n_res_layers):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self.n_res_layers = n_res_layers\n",
    "        self.stack = nn.ModuleList(\n",
    "            [ResidualLayer(in_dim, h_dim, res_h_dim)]*n_res_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.stack:\n",
    "            x = layer(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Discretization bottleneck part of the VQ-VAE.\n",
    "\n",
    "    Inputs:\n",
    "    - n_e : number of embeddings\n",
    "    - e_dim : dimension of embedding\n",
    "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, e_dim, beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.n_e = n_e\n",
    "        self.e_dim = e_dim\n",
    "        self.beta = beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Inputs the output of the encoder network z and maps it to a discrete \n",
    "        one-hot vector that is the index of the closest embedding vector e_j\n",
    "\n",
    "        z (continuous) -> z_q (discrete)\n",
    "\n",
    "        z.shape = (batch, channel, height, width)\n",
    "\n",
    "        quantization pipeline:\n",
    "\n",
    "            1. get encoder input (B,C,H,W)\n",
    "            2. flatten input to (B*H*W,C)\n",
    "\n",
    "        \"\"\"\n",
    "        # reshape z -> (batch, height, width, channel) and flatten\n",
    "        z = z.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flattened = z.view(-1, self.e_dim)\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "\n",
    "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
    "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
    "            torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "\n",
    "        # find closest encodings\n",
    "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
    "        min_encodings = torch.zeros(\n",
    "            min_encoding_indices.shape[0], self.n_e).to(device)\n",
    "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
    "\n",
    "        # compute loss for embedding\n",
    "        loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
    "            torch.mean((z_q - z.detach()) ** 2)\n",
    "\n",
    "        # preserve gradients\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # perplexity\n",
    "        e_mean = torch.mean(min_encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
    "\n",
    "        # reshape back to match original input shape\n",
    "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return loss, z_q, perplexity, min_encodings, min_encoding_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the q_theta (z|x) network. Given a data sample x q_theta \n",
    "    maps to the latent space x -> z.\n",
    "\n",
    "    For a VQ VAE, q_theta outputs parameters of a categorical distribution.\n",
    "\n",
    "    Inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        kernel = (2, 4)\n",
    "        stride = (1, 1)\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, h_dim // 2, \n",
    "                      kernel_size=kernel,\n",
    "                      stride=stride, \n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim // 2, h_dim, \n",
    "                      kernel_size=kernel,\n",
    "                      stride=stride, \n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim, h_dim, \n",
    "                      kernel_size=(kernel[0] - 1, kernel[1] - 1),\n",
    "                      stride=stride, \n",
    "                      padding=1),\n",
    "            ResidualStack(\n",
    "                h_dim, h_dim, res_h_dim, n_res_layers)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_stack(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the p_phi (x|z) network. Given a latent sample z p_phi \n",
    "    maps back to the original space z -> x.\n",
    "\n",
    "    Inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        kernel = (2, 4)\n",
    "        stride = (1, 1)\n",
    "\n",
    "        self.inverse_conv_stack = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_dim, h_dim, \n",
    "                kernel_size=(kernel[0] - 1, kernel[1]-1), \n",
    "                stride=stride, \n",
    "                padding=1),\n",
    "            ResidualStack(h_dim, h_dim, res_h_dim, n_res_layers),\n",
    "            nn.ConvTranspose2d(h_dim, h_dim // 2,\n",
    "                               \n",
    "                               kernel_size=kernel, \n",
    "                               stride=stride, \n",
    "                               padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(h_dim//2, 1, kernel_size=kernel,\n",
    "                               stride=stride, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inverse_conv_stack(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, h_dim, res_h_dim, n_res_layers,\n",
    "                 n_embeddings, embedding_dim, beta, save_img_embedding_map=False):\n",
    "        super(VQVAE, self).__init__()\n",
    "        # encode image into continuous latent space\n",
    "        self.encoder = Encoder(1, h_dim, n_res_layers, res_h_dim)\n",
    "        self.pre_quantization_conv = nn.Conv2d(\n",
    "            h_dim, embedding_dim, kernel_size=1, stride=1)\n",
    "        # pass continuous latent vector through discretization bottleneck\n",
    "        self.vector_quantization = VectorQuantizer(\n",
    "            n_embeddings, embedding_dim, beta)\n",
    "        # decode the discrete latent representation\n",
    "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
    "        # Decoder(128, 128, 3, 64)\n",
    "\n",
    "        if save_img_embedding_map:\n",
    "            self.img_to_embedding_map = {i: [] for i in range(n_embeddings)}\n",
    "        else:\n",
    "            self.img_to_embedding_map = None\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        z_e = self.encoder(x.float())\n",
    "\n",
    "        z_e = self.pre_quantization_conv(z_e)\n",
    "        embedding_loss, z_q, perplexity, _, _ = self.vector_quantization(\n",
    "            z_e)\n",
    "        x_hat = self.decoder(z_q)\n",
    "\n",
    "        if verbose:\n",
    "            print('original data shape:', x.shape)\n",
    "            print('encoded data shape:', z_e.shape)\n",
    "            print('recon data shape:', x_hat.shape)\n",
    "            assert False\n",
    "\n",
    "        return embedding_loss, x_hat, perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE(N_HIDDENS, N_RESIDUAL_HIDDENS,\n",
    "              N_RESIDUAL_LAYERS, N_EMBEDDINGS, EMBEDDING_DIM, BETA).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv_stack): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(2, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(64, 128, kernel_size=(2, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ResidualStack(\n",
       "        (stack): ModuleList(\n",
       "          (0-2): 3 x ResidualLayer(\n",
       "            (res_block): Sequential(\n",
       "              (0): ReLU(inplace=True)\n",
       "              (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_quantization_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (vector_quantization): VectorQuantizer(\n",
       "    (embedding): Embedding(512, 128)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (inverse_conv_stack): Sequential(\n",
       "      (0): ConvTranspose2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ResidualStack(\n",
       "        (stack): ModuleList(\n",
       "          (0-2): 3 x ResidualLayer(\n",
       "            (res_block): Sequential(\n",
       "              (0): ReLU(inplace=True)\n",
       "              (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ConvTranspose2d(128, 64, kernel_size=(2, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): ConvTranspose2d(64, 1, kernel_size=(2, 4), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set up optimizer and training loop\n",
    "\"\"\"\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, amsgrad=True)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'n_updates': 0,\n",
    "    'recon_errors': [],\n",
    "    'loss_vals': [],\n",
    "    'perplexities': [],\n",
    "}\n",
    "\n",
    "def train():\n",
    "    for i in range(N_UPDATES):\n",
    "        (x, _) = next(iter(training_loader))\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embedding_loss, x_hat, perplexity = model(x)\n",
    "        recon_loss = torch.mean((x_hat - x)**2) / x_train_var\n",
    "        loss = recon_loss + embedding_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        results[\"recon_errors\"].append(recon_loss.cpu().detach().numpy())\n",
    "        results[\"perplexities\"].append(perplexity.cpu().detach().numpy())\n",
    "        results[\"loss_vals\"].append(loss.cpu().detach().numpy())\n",
    "        results[\"n_updates\"] = i\n",
    "\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            \"\"\"\n",
    "            save model and print values\n",
    "            \"\"\"\n",
    "            timestamp = time.ctime().replace('  ', ' ').replace(' ', '_').replace(':', '_').lower()\n",
    "\n",
    "            save_model_and_results(model, results, timestamp)\n",
    "\n",
    "            print('Update #', i, 'Recon Error:',\n",
    "                  np.mean(results[\"recon_errors\"][-LOG_INTERVAL:]),\n",
    "                  'Loss', np.mean(results[\"loss_vals\"][-LOG_INTERVAL:]),\n",
    "                  'Perplexity:', np.mean(results[\"perplexities\"][-LOG_INTERVAL:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update # 0 Recon Error: 1.1634601173450887 Loss 1.1684199632605254 Perplexity: 1.3759761\n",
      "Update # 50 Recon Error: 0.8679975829905294 Loss 58.191615171756446 Perplexity: 4.6712666\n",
      "Update # 100 Recon Error: 0.2551946475557978 Loss 126.27030138278529 Perplexity: 4.1741486\n",
      "Update # 150 Recon Error: 0.16647000016888536 Loss 71.41595020951947 Perplexity: 4.707375\n",
      "Update # 200 Recon Error: 0.13823932529595928 Loss 29.491911162148984 Perplexity: 5.352045\n",
      "Update # 250 Recon Error: 0.12502254620372455 Loss 18.42085437908947 Perplexity: 5.1880903\n",
      "Update # 300 Recon Error: 0.12012877583769783 Loss 14.224141570332327 Perplexity: 5.024179\n",
      "Update # 350 Recon Error: 0.1180542216123142 Loss 11.637424415093758 Perplexity: 4.9864593\n",
      "Update # 400 Recon Error: 0.11630004268634946 Loss 9.307494329032783 Perplexity: 4.976264\n",
      "Update # 450 Recon Error: 0.10973685874853718 Loss 7.176224762343508 Perplexity: 4.997649\n",
      "Update # 500 Recon Error: 0.10385890864552617 Loss 5.466221255294696 Perplexity: 4.860225\n",
      "Update # 550 Recon Error: 0.10125129390874368 Loss 4.502636501084951 Perplexity: 5.0703382\n",
      "Update # 600 Recon Error: 0.09732532770651556 Loss 3.7730104616405367 Perplexity: 5.4860334\n",
      "Update # 650 Recon Error: 0.09319319914077882 Loss 3.17017830560898 Perplexity: 5.980898\n",
      "Update # 700 Recon Error: 0.09164805171783748 Loss 2.7280916476040677 Perplexity: 6.69719\n",
      "Update # 750 Recon Error: 0.08946464035130186 Loss 2.3074640223703446 Perplexity: 7.399562\n",
      "Update # 800 Recon Error: 0.08867478549735036 Loss 1.9614759200836847 Perplexity: 7.9465833\n",
      "Update # 850 Recon Error: 0.08551544909743304 Loss 1.6875255561378384 Perplexity: 8.514614\n",
      "Update # 900 Recon Error: 0.0802988642460585 Loss 1.4377228111035112 Perplexity: 9.146939\n",
      "Update # 950 Recon Error: 0.0756599551440882 Loss 1.222121251225631 Perplexity: 9.6897745\n",
      "Update # 1000 Recon Error: 0.07291072616081859 Loss 1.0516132725189271 Perplexity: 10.135716\n",
      "Update # 1050 Recon Error: 0.06887084833792248 Loss 0.8996058797042707 Perplexity: 10.445874\n",
      "Update # 1100 Recon Error: 0.07182785315657048 Loss 0.7919830910697023 Perplexity: 10.996998\n",
      "Update # 1150 Recon Error: 0.06572339669420477 Loss 0.6924831439609743 Perplexity: 11.42277\n",
      "Update # 1200 Recon Error: 0.0642185294816731 Loss 0.6166782117078542 Perplexity: 11.926587\n",
      "Update # 1250 Recon Error: 0.06355471487833801 Loss 0.5596541595060999 Perplexity: 12.538974\n",
      "Update # 1300 Recon Error: 0.06079231295960871 Loss 0.5088394037284227 Perplexity: 13.145726\n",
      "Update # 1350 Recon Error: 0.058319384813148044 Loss 0.46045744276030687 Perplexity: 13.545926\n",
      "Update # 1400 Recon Error: 0.0563909338287005 Loss 0.42517579300915087 Perplexity: 13.970336\n",
      "Update # 1450 Recon Error: 0.05487539362070147 Loss 0.38890244614240715 Perplexity: 14.409646\n",
      "Update # 1500 Recon Error: 0.05277744407501701 Loss 0.3617818819984484 Perplexity: 14.990984\n",
      "Update # 1550 Recon Error: 0.05102116477468388 Loss 0.3351499880550184 Perplexity: 15.556793\n",
      "Update # 1600 Recon Error: 0.05027066768068467 Loss 0.31061538101095354 Perplexity: 16.089096\n",
      "Update # 1650 Recon Error: 0.047832380446609576 Loss 0.28706193527859125 Perplexity: 16.659983\n",
      "Update # 1700 Recon Error: 0.04594242100941742 Loss 0.26685602520930374 Perplexity: 17.30326\n",
      "Update # 1750 Recon Error: 0.044888335163457374 Loss 0.2503725230644774 Perplexity: 17.807167\n",
      "Update # 1800 Recon Error: 0.04345848949399804 Loss 0.2332951912447533 Perplexity: 18.295284\n",
      "Update # 1850 Recon Error: 0.04251206111312181 Loss 0.2183043850600746 Perplexity: 18.841654\n",
      "Update # 1900 Recon Error: 0.04144065065624156 Loss 0.2059608419084731 Perplexity: 19.193016\n",
      "Update # 1950 Recon Error: 0.04096743201458684 Loss 0.19471678798640008 Perplexity: 19.73895\n",
      "Update # 2000 Recon Error: 0.04005979202802251 Loss 0.18538090400512292 Perplexity: 20.066742\n",
      "Update # 2050 Recon Error: 0.039299431155722805 Loss 0.17724225581745454 Perplexity: 20.530798\n",
      "Update # 2100 Recon Error: 0.037895374192714104 Loss 0.16828856308650914 Perplexity: 21.546253\n",
      "Update # 2150 Recon Error: 0.03720167202519639 Loss 0.16077891943620906 Perplexity: 22.391762\n",
      "Update # 2200 Recon Error: 0.036260669839579596 Loss 0.15424548299332333 Perplexity: 22.897049\n",
      "Update # 2250 Recon Error: 0.03530405296152682 Loss 0.14826406268542858 Perplexity: 23.238848\n",
      "Update # 2300 Recon Error: 0.034524533835378804 Loss 0.14231216007205597 Perplexity: 24.380903\n",
      "Update # 2350 Recon Error: 0.03392995807636071 Loss 0.13731584492433357 Perplexity: 25.38727\n",
      "Update # 2400 Recon Error: 0.03324235175665906 Loss 0.13187732179578832 Perplexity: 25.830687\n",
      "Update # 2450 Recon Error: 0.03279802629581244 Loss 0.12756084913244992 Perplexity: 26.60518\n",
      "Update # 2500 Recon Error: 0.0317725963460387 Loss 0.12239800702204119 Perplexity: 27.588577\n",
      "Update # 2550 Recon Error: 0.031524915182382846 Loss 0.11848361083248354 Perplexity: 28.268858\n",
      "Update # 2600 Recon Error: 0.03079983146910893 Loss 0.11445251287942156 Perplexity: 28.86824\n",
      "Update # 2650 Recon Error: 0.03043070681480934 Loss 0.11104986153630785 Perplexity: 29.6709\n",
      "Update # 2700 Recon Error: 0.029574744606488953 Loss 0.10713385974811912 Perplexity: 30.277275\n",
      "Update # 2750 Recon Error: 0.028950578878523498 Loss 0.1040740683776156 Perplexity: 30.896152\n",
      "Update # 2800 Recon Error: 0.028611773026189568 Loss 0.10153788111825919 Perplexity: 31.475754\n",
      "Update # 2850 Recon Error: 0.028130227878559774 Loss 0.09853610660122747 Perplexity: 32.160904\n",
      "Update # 2900 Recon Error: 0.027895144539160758 Loss 0.0958048992028666 Perplexity: 32.53565\n",
      "Update # 2950 Recon Error: 0.027436726059858505 Loss 0.09365558084482403 Perplexity: 32.839996\n",
      "Update # 3000 Recon Error: 0.02712682859061539 Loss 0.0918848936354953 Perplexity: 33.383713\n",
      "Update # 3050 Recon Error: 0.026588177941891927 Loss 0.08924530033037403 Perplexity: 33.811832\n",
      "Update # 3100 Recon Error: 0.026277276558221492 Loss 0.08735047651583067 Perplexity: 34.34583\n",
      "Update # 3150 Recon Error: 0.026150152230717295 Loss 0.08601981713817276 Perplexity: 34.74887\n",
      "Update # 3200 Recon Error: 0.025492177545001885 Loss 0.08379667335646429 Perplexity: 35.19469\n",
      "Update # 3250 Recon Error: 0.025232118501385813 Loss 0.08221849752875246 Perplexity: 35.958023\n",
      "Update # 3300 Recon Error: 0.025175746362039106 Loss 0.08087195319993164 Perplexity: 36.63198\n",
      "Update # 3350 Recon Error: 0.024780433314813815 Loss 0.07973086348451539 Perplexity: 36.910732\n",
      "Update # 3400 Recon Error: 0.024418549499820648 Loss 0.07799798410327333 Perplexity: 37.226665\n",
      "Update # 3450 Recon Error: 0.024340359418625664 Loss 0.07681112818324073 Perplexity: 37.391056\n",
      "Update # 3500 Recon Error: 0.023874642984607498 Loss 0.07541569902155666 Perplexity: 37.552666\n",
      "Update # 3550 Recon Error: 0.023834925169035685 Loss 0.0745573120805471 Perplexity: 37.74645\n",
      "Update # 3600 Recon Error: 0.023446528057790347 Loss 0.07313772198114449 Perplexity: 38.275787\n",
      "Update # 3650 Recon Error: 0.02283588550321204 Loss 0.0716711769931708 Perplexity: 39.243797\n",
      "Update # 3700 Recon Error: 0.022705112598276296 Loss 0.07048063782546536 Perplexity: 39.80326\n",
      "Update # 3750 Recon Error: 0.022240722909761392 Loss 0.06902684791405292 Perplexity: 40.038795\n",
      "Update # 3800 Recon Error: 0.02178922254707734 Loss 0.0677612211194269 Perplexity: 40.164524\n",
      "Update # 3850 Recon Error: 0.02156368524833507 Loss 0.06679383964343852 Perplexity: 40.299065\n",
      "Update # 3900 Recon Error: 0.021308792676529856 Loss 0.06570428214570043 Perplexity: 40.636208\n",
      "Update # 3950 Recon Error: 0.02094572416713447 Loss 0.06437587267925948 Perplexity: 41.124733\n",
      "Update # 4000 Recon Error: 0.02086914337399079 Loss 0.06366597435596062 Perplexity: 41.608746\n",
      "Update # 4050 Recon Error: 0.02066165653154714 Loss 0.06258449998781546 Perplexity: 42.06266\n",
      "Update # 4100 Recon Error: 0.020425541640153907 Loss 0.061923637480369595 Perplexity: 42.441784\n",
      "Update # 4150 Recon Error: 0.020067119487490678 Loss 0.06063131142493728 Perplexity: 43.074566\n",
      "Update # 4200 Recon Error: 0.019722277993452247 Loss 0.059678300419938275 Perplexity: 43.910362\n",
      "Update # 4250 Recon Error: 0.019830525220878293 Loss 0.058849538417108244 Perplexity: 45.213917\n",
      "Update # 4300 Recon Error: 0.01934570697319417 Loss 0.05831041154634861 Perplexity: 45.874283\n",
      "Update # 4350 Recon Error: 0.019082642212115527 Loss 0.05702816003521562 Perplexity: 46.27825\n",
      "Update # 4400 Recon Error: 0.018812247834413362 Loss 0.05626255879482539 Perplexity: 46.295544\n",
      "Update # 4450 Recon Error: 0.018728614284178092 Loss 0.055722741319319084 Perplexity: 46.3456\n",
      "Update # 4500 Recon Error: 0.018394476711483213 Loss 0.054871708541841716 Perplexity: 46.566906\n",
      "Update # 4550 Recon Error: 0.018177130013906827 Loss 0.05409576201459156 Perplexity: 46.893368\n",
      "Update # 4600 Recon Error: 0.018107384364565663 Loss 0.05359237923797016 Perplexity: 47.17424\n",
      "Update # 4650 Recon Error: 0.017927168524169323 Loss 0.053017564108871824 Perplexity: 47.30537\n",
      "Update # 4700 Recon Error: 0.01772455225690279 Loss 0.05230963627799424 Perplexity: 47.518642\n",
      "Update # 4750 Recon Error: 0.01755661030443711 Loss 0.05173486647339863 Perplexity: 47.710148\n",
      "Update # 4800 Recon Error: 0.01758226868030712 Loss 0.05130676735458061 Perplexity: 48.08775\n",
      "Update # 4850 Recon Error: 0.017527338009807162 Loss 0.05096390407952934 Perplexity: 48.339542\n",
      "Update # 4900 Recon Error: 0.017203164136079682 Loss 0.050125254502562416 Perplexity: 48.51547\n",
      "Update # 4950 Recon Error: 0.016776821889994055 Loss 0.04935702582549802 Perplexity: 48.670887\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv_stack): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(2, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(64, 128, kernel_size=(2, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ResidualStack(\n",
       "        (stack): ModuleList(\n",
       "          (0-2): 3 x ResidualLayer(\n",
       "            (res_block): Sequential(\n",
       "              (0): ReLU(inplace=True)\n",
       "              (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_quantization_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (vector_quantization): VectorQuantizer(\n",
       "    (embedding): Embedding(512, 128)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (inverse_conv_stack): Sequential(\n",
       "      (0): ConvTranspose2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ResidualStack(\n",
       "        (stack): ModuleList(\n",
       "          (0-2): 3 x ResidualLayer(\n",
       "            (res_block): Sequential(\n",
       "              (0): ReLU(inplace=True)\n",
       "              (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ConvTranspose2d(128, 64, kernel_size=(2, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): ConvTranspose2d(64, 1, kernel_size=(2, 4), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume `model` is the trained VQVAE instance\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 1, 2, 200)\n",
      "1.0004747225145652\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"results/vqvae_data_sun_jan_12_08_26_21_2025.pth\")['model'])\n",
    "# bbh = np.load('bbh_for_challenge.npy')\n",
    "# backgroundtest = np.load('backgroundtest.npy')\n",
    "\n",
    "\n",
    "# bbhpath = data_folder_path + '/bbh_for_challenge.npy'\n",
    "# bbh = load_hackathon(filepath=data_file_path, train=False)\n",
    "# bbhloader = DataLoader(bbh, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "# x_train_var = np.var(bbhloader.data)\n",
    "# print(x_train_var)\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_batches(data_file_path, batch_size):\n",
    "    # Create a DataLoader for batching\n",
    "    data = load_hackathon(filepath=data_file_path, train=False)\n",
    "    data_loader = DataLoader(bbh, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for batch in data_loader:        \n",
    "        # Run the operations\n",
    "        batch_data = batch[0]\n",
    "        z_e = model.encoder(batch_data.float())  # Continuous latent representation\n",
    "        z_e = model.pre_quantization_conv(z_e)  # Pre-quantized vector\n",
    "        decoded_output = model.decoder(z_e).reshape(batch_data.shape[0], 2, 200)  # Decode and reshape\n",
    "        \n",
    "        outputs.append(decoded_output)\n",
    "\n",
    "    # Concatenate all outputs back to a single tensor\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    return outputs.to('cpu').numpy().reshape(-1, 2, 200)\n",
    "\n",
    "def predict(X, batch_size=8):\n",
    "    \n",
    "    return np.mean((process_batches(X, batch_size=batch_size) - X) ** 2, axis=(1,2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
